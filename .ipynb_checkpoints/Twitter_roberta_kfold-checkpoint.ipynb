{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1603,"status":"ok","timestamp":1702400533909,"user":{"displayName":"Ke Li","userId":"06583527288852565298"},"user_tz":-60},"id":"2nfi7J_zepE0","outputId":"3ba7eeb5-9dde-44c4-c121-6989d7897455"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":247,"status":"ok","timestamp":1702400538662,"user":{"displayName":"Ke Li","userId":"06583527288852565298"},"user_tz":-60},"id":"ODpHxEA0vkd7"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import random\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","import torch.nn as nn\n","from torch.nn import CrossEntropyLoss\n","\n","import transformers\n","from transformers import AutoTokenizer, AutoConfig\n","from transformers import AutoModelForSequenceClassification\n","from transformers import TFAutoModelForSequenceClassification\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n","\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","import re\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1702401051321,"user":{"displayName":"Ke Li","userId":"06583527288852565298"},"user_tz":-60},"id":"pv6KDdCLqngu"},"outputs":[],"source":["FOLDER_PATH = \"/content/drive/MyDrive/Colab Notebooks/Machine learning/ML_project/\"\n","\n","# model parameters\n","MAX_LEN = 512\n","TWITTER_ROBERTA_MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n","TRAIN_BATCH_SIZE = 8\n","VALID_BATCH_SIZE = 4\n","EPOCHS = 2\n","MODEL_PATH = \"fold_model.bin\"\n","tokenizer = AutoTokenizer.from_pretrained(TWITTER_ROBERTA_MODEL)"]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1702401053407,"user":{"displayName":"Ke Li","userId":"06583527288852565298"},"user_tz":-60},"id":"G2QXzlNkvkd9"},"outputs":[],"source":["class ROBERTAModel(nn.Module):\n","    \"\"\"\n","    RoBERTa model with a dropout and linear layer for binary text classification\n","    \"\"\"\n","    def __init__(self, roberta_model, num_classes=2, dropout_rate=0.3):\n","        super(ROBERTAModel, self).__init__()\n","        self.roberta = AutoModelForSequenceClassification.from_pretrained(roberta_model)\n","        self.drop = nn.Dropout(dropout_rate)\n","        self.out = nn.Linear(self.roberta.config.hidden_size, num_classes)\n","\n","    def forward(self, input_ids, attention_mask=None, labels=None):\n","        outputs = self.roberta(input_ids, attention_mask=attention_mask, labels=labels)\n","        logits = outputs.logits\n","        output = self.drop(logits)\n","        return self.out(output)"]},{"cell_type":"code","execution_count":42,"metadata":{"executionInfo":{"elapsed":226,"status":"ok","timestamp":1702401055315,"user":{"displayName":"Ke Li","userId":"06583527288852565298"},"user_tz":-60},"id":"4ykvSgarp9A4"},"outputs":[],"source":["def preprocess(text):\n","  # Remove '<user>'\n","  text = re.sub(r'<user>', '', text)\n","  # Remove '<url>'\n","  text = re.sub(r'<url>', '', text)\n","  # remove numbers\n","  text = re.sub(r'\\d+', '', text)\n","  return text"]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":217,"status":"ok","timestamp":1702401057078,"user":{"displayName":"Ke Li","userId":"06583527288852565298"},"user_tz":-60},"id":"f3qCUn8gvkd9"},"outputs":[],"source":["def train(data, model, optimizer, device):\n","    \"\"\"\n","        Train the model for one epoch\n","    \"\"\"\n","    model.train()\n","    running_loss = 0.0\n","\n","    for batch_idx, d in enumerate(data):\n","        mask = d['mask'].to(device, dtype=torch.long)\n","        ids = d['ids'].to(device, dtype=torch.long)\n","        token_type_ids = d['token_type_ids'].to(device, dtype=torch.long)\n","        targets = d['targets'].to(device, dtype=torch.long)\n","        optimizer.zero_grad()\n","        outputs = model(ids, mask, token_type_ids)\n","\n","        loss = torch.nn.CrossEntropyLoss(outputs, targets) # Calculate loss\n","        loss.backward()\n","        optimizer.step()\n","        if batch_idx % 10 == 0 and batch_idx !=0:\n","            temp = f'Batch index = {batch_idx}\\tRunning Loss = {running_loss/10}'\n","            print(temp)\n","            running_loss = 0.0"]},{"cell_type":"code","execution_count":44,"metadata":{"executionInfo":{"elapsed":2524,"status":"ok","timestamp":1702401061505,"user":{"displayName":"Ke Li","userId":"06583527288852565298"},"user_tz":-60},"id":"6quHUiImvkd9"},"outputs":[],"source":["pos_path = 'data/twitter-datasets/train_pos.txt'\n","neg_path = 'data/twitter-datasets/train_neg.txt'\n","\n","with open(FOLDER_PATH + pos_path, 'r') as f:\n","    pos_tweets = f.readlines()\n","with open(FOLDER_PATH + neg_path, 'r') as f:\n","    neg_tweets = f.readlines()\n","\n","# preprocess data\n","pos_tweets = [preprocess(tweet) for tweet in pos_tweets]\n","neg_tweets = [preprocess(tweet) for tweet in neg_tweets]\n","\n","\n","pos_labels = [1 for _ in range(len(pos_tweets))]\n","neg_labels = [0 for _ in range(len(neg_tweets))]\n","labels = pos_labels + neg_labels\n","tweets = pos_tweets + neg_tweets\n","\n","# Combine labels and tweets into a list of tuples\n","data = list(zip(tweets, labels))\n","\n","# Shuffle the data\n","random.shuffle(data)\n","\n","# Unpack the shuffled data back into separate lists\n","train_tweets, train_labels = zip(*data)\n","\n","# Use RoBERTa tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(TWITTER_ROBERTA_MODEL)\n","config = AutoConfig.from_pretrained(TWITTER_ROBERTA_MODEL)\n","\n","# Tokenize and convert to input IDs\n","def data_generator(tweets, labels, batch_size=32):\n","    for i in range(0, len(tweets), batch_size):\n","        batch_tweets = tweets[i:i + batch_size]\n","        batch_labels = labels[i:i + batch_size]\n","        batch_encodings = tokenizer(batch_tweets, truncation=True, padding=True, max_length=MAX_LEN, return_tensors=\"pt\")\n","        yield batch_encodings, torch.tensor(batch_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R2qrdSaneVVf","outputId":"68ff2909-b78e-428d-b12d-2bf4974b71b5"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Fold 1/5\n","Epoch 1/2\n","Batch 50 - Train loss: 0.40607067942619324, Accuracy: 0.875\n","Batch 100 - Train loss: 0.2631092965602875, Accuracy: 0.84375\n","Batch 150 - Train loss: 0.37577298283576965, Accuracy: 0.875\n","Batch 200 - Train loss: 0.32417452335357666, Accuracy: 0.875\n","Batch 250 - Train loss: 0.2881397306919098, Accuracy: 0.875\n","Batch 300 - Train loss: 0.18938297033309937, Accuracy: 0.9375\n","Batch 350 - Train loss: 0.1849631816148758, Accuracy: 0.90625\n","Batch 400 - Train loss: 0.18197394907474518, Accuracy: 0.9375\n","Batch 450 - Train loss: 0.6610467433929443, Accuracy: 0.75\n","Batch 500 - Train loss: 0.41847431659698486, Accuracy: 0.84375\n","Batch 550 - Train loss: 0.39904776215553284, Accuracy: 0.875\n","Batch 600 - Train loss: 0.24807271361351013, Accuracy: 0.84375\n","Batch 650 - Train loss: 0.2552882730960846, Accuracy: 0.96875\n","Batch 700 - Train loss: 0.32590916752815247, Accuracy: 0.875\n","Batch 750 - Train loss: 0.20261028409004211, Accuracy: 0.90625\n","Batch 800 - Train loss: 0.28382763266563416, Accuracy: 0.90625\n","Batch 850 - Train loss: 0.3038707375526428, Accuracy: 0.875\n","Batch 900 - Train loss: 0.2761252522468567, Accuracy: 0.875\n","Batch 950 - Train loss: 0.3494165241718292, Accuracy: 0.8125\n","Batch 1000 - Train loss: 0.3902705907821655, Accuracy: 0.8125\n","Batch 1050 - Train loss: 0.40393197536468506, Accuracy: 0.84375\n","Batch 1100 - Train loss: 0.3787039518356323, Accuracy: 0.78125\n","Batch 1150 - Train loss: 0.4247402250766754, Accuracy: 0.78125\n","Batch 1200 - Train loss: 0.24673476815223694, Accuracy: 0.90625\n","Batch 1250 - Train loss: 0.2821190357208252, Accuracy: 0.90625\n","Batch 1300 - Train loss: 0.29332512617111206, Accuracy: 0.90625\n","Batch 1350 - Train loss: 0.24833238124847412, Accuracy: 0.90625\n","Batch 1400 - Train loss: 0.2992076873779297, Accuracy: 0.84375\n","Batch 1450 - Train loss: 0.30277058482170105, Accuracy: 0.875\n","Batch 1500 - Train loss: 0.46348991990089417, Accuracy: 0.84375\n","Batch 1550 - Train loss: 0.17014653980731964, Accuracy: 0.96875\n","Batch 1600 - Train loss: 0.3558540344238281, Accuracy: 0.8125\n","Batch 1650 - Train loss: 0.3237989544868469, Accuracy: 0.875\n","Batch 1700 - Train loss: 0.3618922233581543, Accuracy: 0.8125\n","Batch 1750 - Train loss: 0.35510286688804626, Accuracy: 0.90625\n","Batch 1800 - Train loss: 0.26996904611587524, Accuracy: 0.90625\n","Batch 1850 - Train loss: 0.539111316204071, Accuracy: 0.78125\n","Batch 1900 - Train loss: 0.2775723338127136, Accuracy: 0.90625\n","Batch 1950 - Train loss: 0.30272820591926575, Accuracy: 0.84375\n","Batch 2000 - Train loss: 0.2595016062259674, Accuracy: 0.875\n","Batch 2050 - Train loss: 0.30260610580444336, Accuracy: 0.875\n","Batch 2100 - Train loss: 0.281110554933548, Accuracy: 0.9375\n","Batch 2150 - Train loss: 0.23376314342021942, Accuracy: 0.8125\n","Batch 2200 - Train loss: 0.41136765480041504, Accuracy: 0.75\n","Batch 2250 - Train loss: 0.27585700154304504, Accuracy: 0.90625\n","Batch 2300 - Train loss: 0.29754388332366943, Accuracy: 0.90625\n","Batch 2350 - Train loss: 0.18781879544258118, Accuracy: 0.9375\n","Batch 2400 - Train loss: 0.31443965435028076, Accuracy: 0.875\n","Batch 2450 - Train loss: 0.2205517590045929, Accuracy: 0.9375\n","Batch 2500 - Train loss: 0.2247871607542038, Accuracy: 0.90625\n","Batch 2550 - Train loss: 0.38773730397224426, Accuracy: 0.78125\n","Batch 2600 - Train loss: 0.2876797616481781, Accuracy: 0.90625\n","Batch 2650 - Train loss: 0.4459669589996338, Accuracy: 0.8125\n","Batch 2700 - Train loss: 0.27667105197906494, Accuracy: 0.875\n","Batch 2750 - Train loss: 0.17495018243789673, Accuracy: 0.9375\n","Batch 2800 - Train loss: 0.26792070269584656, Accuracy: 0.84375\n","Batch 2850 - Train loss: 0.2270594984292984, Accuracy: 0.875\n","Batch 2900 - Train loss: 0.3468191921710968, Accuracy: 0.90625\n","Batch 2950 - Train loss: 0.4012429416179657, Accuracy: 0.8125\n","Batch 3000 - Train loss: 0.30928704142570496, Accuracy: 0.90625\n","Batch 3050 - Train loss: 0.24776601791381836, Accuracy: 0.875\n","Batch 3100 - Train loss: 0.3960365056991577, Accuracy: 0.78125\n","Batch 3150 - Train loss: 0.418800950050354, Accuracy: 0.84375\n","Batch 3200 - Train loss: 0.2313639372587204, Accuracy: 0.875\n","Batch 3250 - Train loss: 0.2560964822769165, Accuracy: 0.875\n","Batch 3300 - Train loss: 0.2764093577861786, Accuracy: 0.9375\n","Batch 3350 - Train loss: 0.39121541380882263, Accuracy: 0.8125\n","Batch 3400 - Train loss: 0.2932720184326172, Accuracy: 0.84375\n","Batch 3450 - Train loss: 0.2714711129665375, Accuracy: 0.875\n","Batch 3500 - Train loss: 0.31435829401016235, Accuracy: 0.84375\n","Batch 3550 - Train loss: 0.22178369760513306, Accuracy: 0.90625\n","Batch 3600 - Train loss: 0.30756819248199463, Accuracy: 0.8125\n","Batch 3650 - Train loss: 0.25210729241371155, Accuracy: 0.9375\n","Batch 3700 - Train loss: 0.23489783704280853, Accuracy: 0.9375\n","Batch 3750 - Train loss: 0.2054712176322937, Accuracy: 0.90625\n","Batch 3800 - Train loss: 0.372898668050766, Accuracy: 0.875\n","Batch 3850 - Train loss: 0.281512975692749, Accuracy: 0.8125\n","Batch 3900 - Train loss: 0.30551794171333313, Accuracy: 0.875\n","Batch 3950 - Train loss: 0.3046402037143707, Accuracy: 0.84375\n","Batch 4000 - Train loss: 0.30943822860717773, Accuracy: 0.875\n","Batch 4050 - Train loss: 0.1869037002325058, Accuracy: 0.96875\n","Batch 4100 - Train loss: 0.36898958683013916, Accuracy: 0.8125\n","Batch 4150 - Train loss: 0.33060380816459656, Accuracy: 0.8125\n","Batch 4200 - Train loss: 0.08115324378013611, Accuracy: 1.0\n","Batch 4250 - Train loss: 0.17465868592262268, Accuracy: 0.9375\n","Batch 4300 - Train loss: 0.2560384273529053, Accuracy: 0.90625\n","Batch 4350 - Train loss: 0.3328964114189148, Accuracy: 0.875\n","Batch 4400 - Train loss: 0.7332608699798584, Accuracy: 0.71875\n","Batch 4450 - Train loss: 0.3557913303375244, Accuracy: 0.84375\n","Batch 4500 - Train loss: 0.25336188077926636, Accuracy: 0.90625\n","Batch 4550 - Train loss: 0.28845494985580444, Accuracy: 0.875\n","Batch 4600 - Train loss: 0.2523616850376129, Accuracy: 0.90625\n","Batch 4650 - Train loss: 0.23552916944026947, Accuracy: 0.9375\n","Batch 4700 - Train loss: 0.21262170374393463, Accuracy: 0.90625\n","Batch 4750 - Train loss: 0.26125794649124146, Accuracy: 0.875\n","Batch 4800 - Train loss: 0.23005267977714539, Accuracy: 0.90625\n","Batch 4850 - Train loss: 0.3402380347251892, Accuracy: 0.90625\n","Batch 4900 - Train loss: 0.2605357766151428, Accuracy: 0.9375\n","Batch 4950 - Train loss: 0.18171252310276031, Accuracy: 0.9375\n","Batch 5000 - Train loss: 0.40912318229675293, Accuracy: 0.84375\n","Fold 1 - Train loss: 0.009496057345089503, Accuracy: 0.027166796875\n","Epoch 2/2\n","Batch 50 - Train loss: 0.29091110825538635, Accuracy: 0.8125\n","Batch 100 - Train loss: 0.19445891678333282, Accuracy: 0.90625\n","Batch 150 - Train loss: 0.23949654400348663, Accuracy: 0.90625\n","Batch 200 - Train loss: 0.27620649337768555, Accuracy: 0.875\n","Batch 250 - Train loss: 0.28091657161712646, Accuracy: 0.875\n","Batch 300 - Train loss: 0.16946791112422943, Accuracy: 0.9375\n","Batch 350 - Train loss: 0.07861210405826569, Accuracy: 0.96875\n","Batch 400 - Train loss: 0.27911531925201416, Accuracy: 0.875\n","Batch 450 - Train loss: 0.3739568889141083, Accuracy: 0.75\n","Batch 500 - Train loss: 0.1822897493839264, Accuracy: 0.90625\n","Batch 550 - Train loss: 0.49265483021736145, Accuracy: 0.84375\n","Batch 600 - Train loss: 0.13970007002353668, Accuracy: 1.0\n","Batch 650 - Train loss: 0.1597106158733368, Accuracy: 0.96875\n","Batch 700 - Train loss: 0.19868499040603638, Accuracy: 0.96875\n","Batch 750 - Train loss: 0.2615097463130951, Accuracy: 0.90625\n","Batch 800 - Train loss: 0.08988244831562042, Accuracy: 0.9375\n","Batch 850 - Train loss: 0.25895193219184875, Accuracy: 0.90625\n","Batch 900 - Train loss: 0.17159797251224518, Accuracy: 0.90625\n","Batch 950 - Train loss: 0.3246217668056488, Accuracy: 0.875\n","Batch 1000 - Train loss: 0.18792149424552917, Accuracy: 0.9375\n","Batch 1050 - Train loss: 0.2464284747838974, Accuracy: 0.875\n","Batch 1100 - Train loss: 0.2787759304046631, Accuracy: 0.875\n","Batch 1150 - Train loss: 0.23642702400684357, Accuracy: 0.90625\n","Batch 1200 - Train loss: 0.1228666827082634, Accuracy: 1.0\n","Batch 1250 - Train loss: 0.1578471064567566, Accuracy: 1.0\n","Batch 1300 - Train loss: 0.2287423014640808, Accuracy: 0.90625\n","Batch 1350 - Train loss: 0.1767968386411667, Accuracy: 0.90625\n","Batch 1400 - Train loss: 0.2839999198913574, Accuracy: 0.875\n","Batch 1450 - Train loss: 0.14621412754058838, Accuracy: 0.96875\n","Batch 1500 - Train loss: 0.4504493474960327, Accuracy: 0.875\n","Batch 1550 - Train loss: 0.16294191777706146, Accuracy: 0.96875\n","Batch 1600 - Train loss: 0.2461635321378708, Accuracy: 0.875\n","Batch 1650 - Train loss: 0.3096221089363098, Accuracy: 0.78125\n","Batch 1700 - Train loss: 0.32062432169914246, Accuracy: 0.84375\n","Batch 1750 - Train loss: 0.21595457196235657, Accuracy: 0.875\n","Batch 1800 - Train loss: 0.18477581441402435, Accuracy: 0.875\n","Batch 1850 - Train loss: 0.5860617160797119, Accuracy: 0.75\n","Batch 1900 - Train loss: 0.13847164809703827, Accuracy: 0.96875\n","Batch 1950 - Train loss: 0.460951030254364, Accuracy: 0.78125\n","Batch 2000 - Train loss: 0.09830192476511002, Accuracy: 0.96875\n","Batch 2050 - Train loss: 0.269457072019577, Accuracy: 0.875\n","Batch 2100 - Train loss: 0.16359204053878784, Accuracy: 0.9375\n","Batch 2150 - Train loss: 0.14033466577529907, Accuracy: 0.9375\n","Batch 2200 - Train loss: 0.26826339960098267, Accuracy: 0.8125\n","Batch 2250 - Train loss: 0.21983234584331512, Accuracy: 0.90625\n","Batch 2300 - Train loss: 0.22336283326148987, Accuracy: 0.90625\n","Batch 2350 - Train loss: 0.15712781250476837, Accuracy: 0.9375\n","Batch 2400 - Train loss: 0.25937965512275696, Accuracy: 0.90625\n","Batch 2450 - Train loss: 0.1932939738035202, Accuracy: 0.875\n","Batch 2500 - Train loss: 0.17037992179393768, Accuracy: 0.90625\n","Batch 2550 - Train loss: 0.2738193869590759, Accuracy: 0.90625\n","Batch 2600 - Train loss: 0.2511288523674011, Accuracy: 0.90625\n","Batch 2650 - Train loss: 0.280187726020813, Accuracy: 0.875\n","Batch 2700 - Train loss: 0.2585326135158539, Accuracy: 0.90625\n","Batch 2750 - Train loss: 0.1361321061849594, Accuracy: 0.96875\n","Batch 2800 - Train loss: 0.26001042127609253, Accuracy: 0.90625\n","Batch 2850 - Train loss: 0.11860170215368271, Accuracy: 0.9375\n","Batch 2900 - Train loss: 0.2204485833644867, Accuracy: 0.9375\n","Batch 2950 - Train loss: 0.3337201178073883, Accuracy: 0.875\n","Batch 3000 - Train loss: 0.1964876353740692, Accuracy: 0.90625\n","Batch 3050 - Train loss: 0.13030652701854706, Accuracy: 0.9375\n","Batch 3100 - Train loss: 0.39806511998176575, Accuracy: 0.75\n","Batch 3150 - Train loss: 0.6601988077163696, Accuracy: 0.5625\n","Batch 3200 - Train loss: 0.5048295259475708, Accuracy: 0.8125\n","Batch 3250 - Train loss: 0.3175036907196045, Accuracy: 0.9375\n","Batch 3300 - Train loss: 0.2231656312942505, Accuracy: 0.90625\n","Batch 3350 - Train loss: 0.3212890326976776, Accuracy: 0.875\n","Batch 3400 - Train loss: 0.2834091782569885, Accuracy: 0.875\n","Batch 3450 - Train loss: 0.20401541888713837, Accuracy: 0.9375\n","Batch 3500 - Train loss: 0.262830525636673, Accuracy: 0.9375\n","Batch 3550 - Train loss: 0.13006901741027832, Accuracy: 0.90625\n","Batch 3600 - Train loss: 0.3598165810108185, Accuracy: 0.84375\n","Batch 3650 - Train loss: 0.22614453732967377, Accuracy: 0.9375\n","Batch 3700 - Train loss: 0.1517704576253891, Accuracy: 0.96875\n","Batch 3750 - Train loss: 0.18554791808128357, Accuracy: 0.9375\n","Batch 3800 - Train loss: 0.29609766602516174, Accuracy: 0.875\n","Batch 3850 - Train loss: 0.206996351480484, Accuracy: 0.9375\n","Batch 3900 - Train loss: 0.27299126982688904, Accuracy: 0.875\n","Batch 3950 - Train loss: 0.3225363790988922, Accuracy: 0.875\n","Batch 4000 - Train loss: 0.26976338028907776, Accuracy: 0.875\n","Batch 4050 - Train loss: 0.281676709651947, Accuracy: 0.9375\n","Batch 4100 - Train loss: 0.2778342664241791, Accuracy: 0.84375\n","Batch 4150 - Train loss: 0.2743968367576599, Accuracy: 0.96875\n","Batch 4200 - Train loss: 0.058001767843961716, Accuracy: 1.0\n","Batch 4250 - Train loss: 0.2079695612192154, Accuracy: 0.90625\n","Batch 4300 - Train loss: 0.27500343322753906, Accuracy: 0.90625\n","Batch 4350 - Train loss: 0.2615027129650116, Accuracy: 0.90625\n","Batch 4400 - Train loss: 0.2882060110569, Accuracy: 0.875\n","Batch 4450 - Train loss: 0.21406210958957672, Accuracy: 0.875\n","Batch 4500 - Train loss: 0.16245123744010925, Accuracy: 0.90625\n","Batch 4550 - Train loss: 0.23186764121055603, Accuracy: 0.90625\n","Batch 4600 - Train loss: 0.16933949291706085, Accuracy: 0.96875\n","Batch 4650 - Train loss: 0.11600378155708313, Accuracy: 0.96875\n","Batch 4700 - Train loss: 0.12949605286121368, Accuracy: 0.90625\n","Batch 4750 - Train loss: 0.34834417700767517, Accuracy: 0.875\n","Batch 4800 - Train loss: 0.2083899974822998, Accuracy: 0.90625\n","Batch 4850 - Train loss: 0.40875425934791565, Accuracy: 0.84375\n","Batch 4900 - Train loss: 0.2570922076702118, Accuracy: 0.90625\n","Batch 4950 - Train loss: 0.08555013686418533, Accuracy: 1.0\n","Batch 5000 - Train loss: 0.3318835198879242, Accuracy: 0.875\n","Fold 1 - Train loss: 0.007543169012200087, Accuracy: 0.028061328125\n","Fold 1 - Validation loss: 0.008821469470672309, Accuracy: 0.0275\n","Fold 2/5\n","Epoch 1/2\n","Batch 50 - Train loss: 0.1670377254486084, Accuracy: 0.875\n","Batch 100 - Train loss: 0.11753726005554199, Accuracy: 0.96875\n","Batch 150 - Train loss: 0.12591537833213806, Accuracy: 0.9375\n","Batch 200 - Train loss: 0.17484083771705627, Accuracy: 0.90625\n","Batch 250 - Train loss: 0.2674402892589569, Accuracy: 0.9375\n","Batch 300 - Train loss: 0.136246919631958, Accuracy: 0.96875\n","Batch 350 - Train loss: 0.2694820165634155, Accuracy: 0.875\n","Batch 400 - Train loss: 0.0963568165898323, Accuracy: 1.0\n","Batch 450 - Train loss: 0.10518957674503326, Accuracy: 1.0\n","Batch 500 - Train loss: 0.23013505339622498, Accuracy: 0.875\n","Batch 550 - Train loss: 0.23526202142238617, Accuracy: 0.8125\n","Batch 600 - Train loss: 0.12212006747722626, Accuracy: 0.9375\n","Batch 650 - Train loss: 0.28941917419433594, Accuracy: 0.84375\n","Batch 700 - Train loss: 0.1603282243013382, Accuracy: 0.9375\n","Batch 750 - Train loss: 0.3102945387363434, Accuracy: 0.90625\n","Batch 800 - Train loss: 0.1913825273513794, Accuracy: 0.90625\n","Batch 850 - Train loss: 0.22383758425712585, Accuracy: 0.9375\n","Batch 900 - Train loss: 0.5226514339447021, Accuracy: 0.8125\n","Batch 950 - Train loss: 0.23936934769153595, Accuracy: 0.875\n","Batch 1000 - Train loss: 0.35558074712753296, Accuracy: 0.875\n","Batch 1050 - Train loss: 0.15782970190048218, Accuracy: 0.90625\n","Batch 1100 - Train loss: 0.1216917335987091, Accuracy: 0.96875\n","Batch 1150 - Train loss: 0.32769477367401123, Accuracy: 0.84375\n","Batch 1200 - Train loss: 0.2530454993247986, Accuracy: 0.90625\n","Batch 1250 - Train loss: 0.14988228678703308, Accuracy: 0.9375\n","Batch 1300 - Train loss: 0.2688671350479126, Accuracy: 0.875\n","Batch 1350 - Train loss: 0.06463413685560226, Accuracy: 1.0\n","Batch 1400 - Train loss: 0.14348289370536804, Accuracy: 0.9375\n","Batch 1450 - Train loss: 0.07542289048433304, Accuracy: 0.96875\n","Batch 1500 - Train loss: 0.291820764541626, Accuracy: 0.90625\n","Batch 1550 - Train loss: 0.32751429080963135, Accuracy: 0.84375\n","Batch 1600 - Train loss: 0.06830421835184097, Accuracy: 1.0\n","Batch 1650 - Train loss: 0.093851238489151, Accuracy: 0.96875\n","Batch 1700 - Train loss: 0.2715362310409546, Accuracy: 0.90625\n","Batch 1750 - Train loss: 0.24812529981136322, Accuracy: 0.875\n","Batch 1800 - Train loss: 0.16184434294700623, Accuracy: 0.9375\n","Batch 1850 - Train loss: 0.16304653882980347, Accuracy: 0.9375\n","Batch 1900 - Train loss: 0.08969090133905411, Accuracy: 0.96875\n","Batch 1950 - Train loss: 0.21186886727809906, Accuracy: 0.9375\n","Batch 2000 - Train loss: 0.1617370843887329, Accuracy: 0.96875\n","Batch 2050 - Train loss: 0.20223744213581085, Accuracy: 0.90625\n","Batch 2100 - Train loss: 0.44145023822784424, Accuracy: 0.875\n","Batch 2150 - Train loss: 0.23237797617912292, Accuracy: 0.9375\n","Batch 2200 - Train loss: 0.40116581320762634, Accuracy: 0.8125\n","Batch 2250 - Train loss: 0.37381449341773987, Accuracy: 0.875\n","Batch 2300 - Train loss: 0.3955897390842438, Accuracy: 0.84375\n","Batch 2350 - Train loss: 0.23618313670158386, Accuracy: 0.84375\n","Batch 2400 - Train loss: 0.19845527410507202, Accuracy: 0.90625\n","Batch 2450 - Train loss: 0.35991933941841125, Accuracy: 0.875\n","Batch 2500 - Train loss: 0.20727001130580902, Accuracy: 0.96875\n","Batch 2550 - Train loss: 0.2307303249835968, Accuracy: 0.90625\n","Batch 2600 - Train loss: 0.30218425393104553, Accuracy: 0.90625\n","Batch 2650 - Train loss: 0.1201886385679245, Accuracy: 0.9375\n","Batch 2700 - Train loss: 0.17112264037132263, Accuracy: 0.90625\n","Batch 2750 - Train loss: 0.10314896702766418, Accuracy: 1.0\n","Batch 2800 - Train loss: 0.09928698092699051, Accuracy: 1.0\n","Batch 2850 - Train loss: 0.17541152238845825, Accuracy: 0.9375\n","Batch 2900 - Train loss: 0.11072365939617157, Accuracy: 0.9375\n","Batch 2950 - Train loss: 0.10218945145606995, Accuracy: 0.96875\n","Batch 3000 - Train loss: 0.17417648434638977, Accuracy: 0.90625\n","Batch 3050 - Train loss: 0.19225408136844635, Accuracy: 0.90625\n","Batch 3100 - Train loss: 0.1767357438802719, Accuracy: 0.90625\n","Batch 3150 - Train loss: 0.13313695788383484, Accuracy: 0.9375\n","Batch 3200 - Train loss: 0.28285878896713257, Accuracy: 0.90625\n","Batch 3250 - Train loss: 0.13841752707958221, Accuracy: 0.96875\n","Batch 3300 - Train loss: 0.13617081940174103, Accuracy: 0.9375\n","Batch 3350 - Train loss: 0.40120813250541687, Accuracy: 0.78125\n","Batch 3400 - Train loss: 0.09485147148370743, Accuracy: 0.96875\n","Batch 3450 - Train loss: 0.1713935136795044, Accuracy: 0.90625\n","Batch 3500 - Train loss: 0.12275931239128113, Accuracy: 0.96875\n","Batch 3550 - Train loss: 0.12456683069467545, Accuracy: 0.96875\n","Batch 3600 - Train loss: 0.14209911227226257, Accuracy: 0.9375\n","Batch 3650 - Train loss: 0.22994364798069, Accuracy: 0.9375\n","Batch 3700 - Train loss: 0.18333382904529572, Accuracy: 0.96875\n","Batch 3750 - Train loss: 0.1838131695985794, Accuracy: 0.9375\n","Batch 3800 - Train loss: 0.12486983835697174, Accuracy: 0.96875\n","Batch 3850 - Train loss: 0.20780856907367706, Accuracy: 0.9375\n","Batch 3900 - Train loss: 0.1661992371082306, Accuracy: 0.9375\n","Batch 3950 - Train loss: 0.15814067423343658, Accuracy: 0.96875\n","Batch 4000 - Train loss: 0.17360985279083252, Accuracy: 0.9375\n","Batch 4050 - Train loss: 0.09079250693321228, Accuracy: 0.96875\n","Batch 4100 - Train loss: 0.16933274269104004, Accuracy: 0.90625\n","Batch 4150 - Train loss: 0.4015060067176819, Accuracy: 0.8125\n"]}],"source":["# Set k value\n","k_folds = 5\n","kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n","\n","model = AutoModelForSequenceClassification.from_pretrained(TWITTER_ROBERTA_MODEL)\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","num_epochs = 2  # Number of training epochs\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model.to(device)\n","criterion = CrossEntropyLoss()\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","scheduler = get_linear_schedule_with_warmup(optimizer,\n","                      num_warmup_steps=0,\n","                      num_training_steps=len(train_labels) * (k_folds-1)*num_epochs)\n","\n","\n","# Function to calculate accuracy\n","def calc_accuracy(preds, labels):\n","    _, predictions = torch.max(preds, dim=1)\n","    correct = (predictions == labels).float()\n","    acc = correct.sum() / len(correct)\n","    return acc\n","\n","all_labels = []\n","all_predictions = []\n","best_accuracy = 0\n","\n","for fold, (train_indices, val_indices) in enumerate(kf.split(train_tweets)):\n","    print(f\"Fold {fold + 1}/{k_folds}\")\n","\n","    # Create data loaders for the current fold\n","    train_fold_tweets = [train_tweets[i] for i in train_indices]\n","    train_fold_labels = [train_labels[i] for i in train_indices]\n","    val_fold_tweets = [train_tweets[i] for i in val_indices]\n","    val_fold_labels = [train_labels[i] for i in val_indices]\n","\n","    # Training Loop\n","    for epoch in range(num_epochs):\n","        print(f\"Epoch {epoch+1}/{num_epochs}\")\n","        model.train()\n","        total_train_loss = 0\n","        total_train_acc = 0\n","\n","        # Use the data generator for training\n","        i = 1\n","        for batch_encodings, batch_labels in data_generator(train_fold_tweets, train_fold_labels, batch_size=32):\n","            batch_encodings = {k: v.to(device) for k, v in batch_encodings.items()}\n","            batch_labels = batch_labels.to(device)\n","\n","            # Forward pass\n","            outputs = model(**batch_encodings)\n","            loss = criterion(outputs.logits, batch_labels)\n","            acc = calc_accuracy(outputs.logits, batch_labels)\n","\n","            # Backward pass\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","            optimizer.zero_grad()\n","\n","            total_train_loss += loss.item()\n","            total_train_acc += acc.item()\n","\n","            if i%50==0:\n","              print(f\"Batch {i} - Train loss: {loss.item()}, Accuracy: {acc.item()}\")\n","            i+=1\n","\n","        avg_train_loss = total_train_loss / len(train_fold_tweets)\n","        avg_train_acc = total_train_acc / len(train_fold_tweets)\n","        print(f\"Fold {fold+1} - Train loss: {avg_train_loss}, Accuracy: {avg_train_acc}\")\n","\n","    # Validation Loop\n","    model.eval()\n","    total_val_accuracy = 0\n","    total_val_loss = 0\n","\n","    for batch_encodings, batch_labels in data_generator(val_fold_tweets, val_fold_labels, batch_size=32):\n","        with torch.no_grad():\n","            batch_encodings = {k: v.to(device) for k, v in batch_encodings.items()}\n","            batch_labels = batch_labels.to(device)\n","\n","            outputs = model(**batch_encodings)\n","            loss = criterion(outputs.logits, batch_labels)\n","            acc = calc_accuracy(outputs.logits, batch_labels)\n","\n","            total_val_loss += loss.item()\n","            total_val_accuracy += acc.item()\n","\n","            # for overall metrics calculation\n","            preds = outputs.logits.argmax(dim=1).cpu().numpy()\n","            labels = batch_labels.cpu().numpy()\n","            all_predictions.extend(preds)\n","            all_labels.extend(labels)\n","\n","    avg_val_loss = total_val_loss / len(val_fold_tweets)\n","    avg_val_accuracy = total_val_accuracy / len(val_fold_tweets)\n","    print(f\"Fold {fold+1} - Validation loss: {avg_val_loss}, Accuracy: {avg_val_accuracy}\")\n","    if avg_val_accuracy>best_accuracy:\n","      torch.save(model, FOLDER_PATH + 'manipulated/roberta.pth')\n","\n","# After all folds, calculate overall metrics\n","precision = precision_score(all_labels, all_predictions, average='macro')\n","recall = recall_score(all_labels, all_predictions, average='macro')\n","f1 = f1_score(all_labels, all_predictions, average='macro')\n","\n","print(\"Training complete!\")\n","print(f\"Overall Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")"]},{"cell_type":"markdown","metadata":{"id":"EFuJT7aSioEc"},"source":["### Possible improvement\n","1. Don't store all the scores in one variable, just store for each epoch and only keep the average score\n","2. save model for Each epoch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1phcjpJ7vkd-"},"outputs":[],"source":["torch.save(model, FOLDER_PATH + 'manipulated/roberta_final.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kvt6507Ha_sp"},"outputs":[],"source":["## predict\n","model = torch.load(FOLDER_PATH + 'manipulated/bert.pth')\n","\n","## use the test set\n","test_path = FOLDER_PATH +'data/twitter-datasets/test_data.txt'\n","with open(test_path, 'r') as f:\n","    test_tweets = f.readlines()\n","\n","test_encodings = tokenizer(test_tweets, truncation=True, padding=True, max_length=MAX_LEN)\n","test_dataset = TweetDataset(test_encodings, [0 for _ in range(len(test_tweets))])\n","test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n","\n","\n","model.eval()\n","predictions = []\n","for batch in test_loader:\n","    with torch.no_grad():\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        outputs = model(**batch)\n","        preds = outputs.logits.argmax(dim=1).cpu().numpy()\n","        predictions.extend(preds)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b6r6SXzwbgee"},"outputs":[],"source":["predictions = np.array(predictions)\n","predictions[predictions == 0] = -1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m_iHW_xrbVlm"},"outputs":[],"source":["print(predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zG1C1sVVbKZp"},"outputs":[],"source":["submission = pd.DataFrame({'Id':range(1, len(predictions) + 1),'Prediction': predictions})\n","submission.to_csv(FOLDER_PATH + 'manipulated/roberta_submission.csv', index=False)"]}],"metadata":{"accelerator":"TPU","colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
