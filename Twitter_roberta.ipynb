{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":327,"status":"ok","timestamp":1702377237005,"user":{"displayName":"Ke Li","userId":"06583527288852565298"},"user_tz":-60},"id":"pv6KDdCLqngu"},"outputs":[],"source":["MAX_LEN = 512\n","TWITTER_ROBERTA_MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n","FOLDER_PATH = \"/content/drive/MyDrive/Colab Notebooks/Machine learning/ML_project/\"\n","# reduce lr on plateau"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1702379269677,"user":{"displayName":"Ke Li","userId":"06583527288852565298"},"user_tz":-60},"id":"ODpHxEA0vkd7"},"outputs":[{"name":"stderr","output_type":"stream","text":["d:\\APPS\\Anaconda\\envs\\ML\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import random\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","import torch.nn as nn\n","from torch.nn import CrossEntropyLoss\n","\n","import transformers\n","from transformers import AutoTokenizer, AutoConfig\n","from transformers import AutoModelForSequenceClassification\n","from transformers import TFAutoModelForSequenceClassification\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n","\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","import re\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1702376615599,"user":{"displayName":"Ke Li","userId":"06583527288852565298"},"user_tz":-60},"id":"G2QXzlNkvkd9"},"outputs":[],"source":["class ROBERTAModel(nn.Module):\n","    \"\"\"\n","    RoBERTa model with a dropout and linear layer for binary text classification\n","    \"\"\"\n","    def __init__(self, roberta_model, num_classes=2, dropout_rate=0.3):\n","        super(ROBERTAModel, self).__init__()\n","        self.roberta = AutoModelForSequenceClassification.from_pretrained(roberta_model)\n","        self.drop = nn.Dropout(dropout_rate)\n","        self.out = nn.Linear(self.roberta.config.hidden_size, num_classes)\n","\n","    def forward(self, input_ids, attention_mask=None, labels=None):\n","        outputs = self.roberta(input_ids, attention_mask=attention_mask, labels=labels)\n","        logits = outputs.logits\n","        output = self.drop(logits)\n","        return self.out(output)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":305,"status":"ok","timestamp":1702377187028,"user":{"displayName":"Ke Li","userId":"06583527288852565298"},"user_tz":-60},"id":"4ykvSgarp9A4"},"outputs":[],"source":["def preprocess(text):\n","  # Remove '<user>'\n","  text = re.sub(r'<user>', '', text)\n","  # Remove '<url>'\n","  text = re.sub(r'<url>', '', text)\n","  # remove numbers\n","  text = re.sub(r'\\d+', '', text)\n","  return text"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":328,"status":"ok","timestamp":1702377195226,"user":{"displayName":"Ke Li","userId":"06583527288852565298"},"user_tz":-60},"id":"f3qCUn8gvkd9"},"outputs":[],"source":["def train(data, model, optimizer, device):\n","    \"\"\"\n","        Train the model for one epoch\n","    \"\"\"\n","    model.train()\n","    running_loss = 0.0\n","\n","    for batch_idx, d in enumerate(data):\n","        mask = d['mask'].to(device, dtype=torch.long)\n","        ids = d['ids'].to(device, dtype=torch.long)\n","        token_type_ids = d['token_type_ids'].to(device, dtype=torch.long)\n","        targets = d['targets'].to(device, dtype=torch.long)\n","        optimizer.zero_grad()\n","        outputs = model(ids, mask, token_type_ids)\n","\n","        loss = torch.nn.CrossEntropyLoss(outputs, targets) # Calculate loss\n","        loss.backward()\n","        optimizer.step()\n","        if batch_idx % 10 == 0 and batch_idx !=0:\n","            temp = f'Batch index = {batch_idx}\\tRunning Loss = {running_loss/10}'\n","            print(temp)\n","            running_loss = 0.0"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":24577,"status":"ok","timestamp":1702379310590,"user":{"displayName":"Ke Li","userId":"06583527288852565298"},"user_tz":-60},"id":"6quHUiImvkd9"},"outputs":[{"name":"stderr","output_type":"stream","text":["config.json: 100%|██████████| 929/929 [00:00<?, ?B/s] \n","d:\\APPS\\Anaconda\\envs\\ML\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Ke\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n","To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n","  warnings.warn(message)\n","vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 3.15MB/s]\n","merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 4.71MB/s]\n","special_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 232kB/s]\n"]}],"source":["pos_path = 'data/twitter-datasets/train_pos_full.txt'\n","neg_path = 'data/twitter-datasets/train_neg_full.txt'\n","\n","with open(pos_path, 'r') as f:\n","    pos_tweets = f.readlines()\n","with open(neg_path, 'r') as f:\n","    neg_tweets = f.readlines()\n","\n","# TODO: preprocess data\n","pos_tweets = [preprocess(tweet) for tweet in pos_tweets]\n","neg_tweets = [preprocess(tweet) for tweet in neg_tweets]\n","\n","\n","pos_labels = [1 for _ in range(len(pos_tweets))]\n","neg_labels = [0 for _ in range(len(neg_tweets))]\n","labels = pos_labels + neg_labels\n","tweets = pos_tweets + neg_tweets\n","\n","# Combine labels and tweets into a list of tuples\n","data = list(zip(tweets, labels))\n","\n","# Shuffle the data\n","random.shuffle(data)\n","\n","# Unpack the shuffled data back into separate lists\n","train_tweets, train_labels = zip(*data)\n","\n","# Use RoBERTa tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(TWITTER_ROBERTA_MODEL)\n","\n","# Tokenize and convert to input IDs\n","train_encodings = tokenizer(train_tweets, truncation=True, padding=True, max_length=MAX_LEN, return_tensors=\"pt\")"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":222,"status":"ok","timestamp":1702379317704,"user":{"displayName":"Ke Li","userId":"06583527288852565298"},"user_tz":-60},"id":"5HZcRenZioEa"},"outputs":[],"source":["class TweetDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = TweetDataset(train_encodings, train_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Set k value\n","k_folds = 5\n","kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n","\n","num_epoch = 1\n","\n","model = AutoModelForSequenceClassification.from_pretrained(TWITTER_ROBERTA_MODEL)\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","num_epochs = 3  # Number of training epochs\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model.to(device)\n","criterion = CrossEntropyLoss()\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                            num_warmup_steps=0,\n","                                            num_training_steps=len(train_dataset) * (k_folds-1))\n","\n","\n","# Function to calculate accuracy\n","def calc_accuracy(preds, labels):\n","    _, predictions = torch.max(preds, dim=1)\n","    correct = (predictions == labels).float()\n","    acc = correct.sum() / len(correct)\n","    return acc\n","\n","all_labels = []\n","all_predictions = []\n","\n","for fold, (train_indices, val_indices) in enumerate(kf.split(train_dataset)):\n","    print(f\"Fold {fold + 1}/{k_folds}\")\n","\n","    # Create data loaders for the current fold\n","    train_fold_subset = torch.utils.data.Subset(train_dataset, train_indices)\n","    val_fold_subset = torch.utils.data.Subset(train_dataset, val_indices)\n","    train_loader = DataLoader(train_fold_subset, batch_size=32, shuffle=True)\n","    val_loader = DataLoader(val_fold_subset, batch_size=32, shuffle=False)\n","\n","    for epoch in range(num_epochs):\n","        print(f\"Epoch {epoch+1}/{num_epochs}\")\n","        model.train()\n","        total_train_loss = 0\n","        total_train_acc = 0\n","\n","        for step, batch in enumerate(train_loader):\n","            batch = {k: v.to(device) for k, v in batch.items()}\n","            outputs = model(**batch)\n","            loss = criterion(outputs.logits, batch['labels'])\n","            acc = calc_accuracy(outputs.logits, batch['labels'])\n","            print(f\"Step {step+1}/{len(train_loader)} - Loss: {loss.item()}, Accuracy: {acc.item()}\")\n","            total_train_loss += loss.item()\n","            total_train_acc += acc.item()\n","\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","            optimizer.zero_grad()\n","\n","        avg_train_loss = total_train_loss / len(train_loader)\n","        avg_train_acc = total_train_acc / len(train_loader)\n","        print(f\"Epoch {epoch+1} - Train loss: {avg_train_loss}, Accuracy: {avg_train_acc}\")\n","\n","        # Validation Loop\n","        model.eval()\n","        total_val_accuracy = 0\n","        total_val_loss = 0\n","\n","        for batch in val_loader:\n","            with torch.no_grad():\n","                batch = {k: v.to(device) for k, v in batch.items()}\n","                outputs = model(**batch)\n","                loss = criterion(outputs.logits, batch['labels'])\n","                acc = calc_accuracy(outputs.logits, batch['labels'])\n","\n","                total_val_loss += loss.item()\n","                total_val_accuracy += acc.item()\n","\n","                # for overall metrics calculation\n","                preds = outputs.logits.argmax(dim=1).cpu().numpy()\n","                labels = batch['labels'].cpu().numpy()\n","                all_predictions.extend(preds)\n","                all_labels.extend(labels)\n","\n","        avg_val_loss = total_val_loss / len(val_loader)\n","        avg_val_accuracy = total_val_accuracy / len(val_loader)\n","        print(f\"Epoch {epoch+1} - Validation loss: {avg_val_loss}, Accuracy: {avg_val_accuracy}\")\n","\n","# After all folds, calculate overall metrics\n","precision = precision_score(all_labels, all_predictions, average='macro')\n","recall = recall_score(all_labels, all_predictions, average='macro')\n","f1 = f1_score(all_labels, all_predictions, average='macro')\n","\n","print(\"Training complete!\")\n","print(f\"Overall Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")"]},{"cell_type":"markdown","metadata":{"id":"EFuJT7aSioEc"},"source":["### Possible improvement\n","1. Don't store all the scores in one variable, just store for each epoch and only keep the average score\n","2. save model for Each epoch"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":1882,"status":"ok","timestamp":1702388325192,"user":{"displayName":"Ke Li","userId":"06583527288852565298"},"user_tz":-60},"id":"1phcjpJ7vkd-"},"outputs":[],"source":["torch.save(model, FOLDER_PATH + 'manipulated/bert.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kvt6507Ha_sp"},"outputs":[],"source":["## predict\n","model = torch.load(FOLDER_PATH + 'manipulated/bert.pth')\n","\n","## use the test set\n","test_path = FOLDER_PATH +'data/twitter-datasets/test_data.txt'\n","with open(test_path, 'r') as f:\n","    test_tweets = f.readlines()\n","\n","test_encodings = tokenizer(test_tweets, truncation=True, padding=True, max_length=MAX_LEN)\n","test_dataset = TweetDataset(test_encodings, [0 for _ in range(len(test_tweets))])\n","test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n","\n","\n","model.eval()\n","predictions = []\n","for batch in test_loader:\n","    with torch.no_grad():\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        outputs = model(**batch)\n","        preds = outputs.logits.argmax(dim=1).cpu().numpy()\n","        predictions.extend(preds)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":100,"status":"aborted","timestamp":1702388325233,"user":{"displayName":"Ke Li","userId":"06583527288852565298"},"user_tz":-60},"id":"b6r6SXzwbgee"},"outputs":[],"source":["predictions = np.array(predictions)\n","predictions[predictions == 0] = -1"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":99,"status":"aborted","timestamp":1702388325233,"user":{"displayName":"Ke Li","userId":"06583527288852565298"},"user_tz":-60},"id":"m_iHW_xrbVlm"},"outputs":[],"source":["print(predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":100,"status":"aborted","timestamp":1702388325234,"user":{"displayName":"Ke Li","userId":"06583527288852565298"},"user_tz":-60},"id":"zG1C1sVVbKZp"},"outputs":[],"source":["submission = pd.DataFrame({'Id':range(1, len(predictions) + 1),'Prediction': predictions})\n","submission.to_csv(FOLDER_PATH + 'manipulated/roberta_submission.csv', index=False)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
