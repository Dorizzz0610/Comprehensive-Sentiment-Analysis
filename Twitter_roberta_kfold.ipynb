{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2nfi7J_zepE0","executionInfo":{"status":"ok","timestamp":1702393934470,"user_tz":-60,"elapsed":2453,"user":{"displayName":"Ke Li","userId":"06583527288852565298"}},"outputId":"ddd9560b-6251-4cd2-b7b8-500aa71d0fb7"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":331,"status":"ok","timestamp":1702394860019,"user":{"displayName":"Ke Li","userId":"06583527288852565298"},"user_tz":-60},"id":"pv6KDdCLqngu"},"outputs":[],"source":["MAX_LEN = 512\n","TWITTER_ROBERTA_MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n","FOLDER_PATH = \"/content/drive/MyDrive/Colab Notebooks/Machine learning/ML_project/\"\n","# reduce lr on plateau"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":490,"status":"ok","timestamp":1702394861753,"user":{"displayName":"Ke Li","userId":"06583527288852565298"},"user_tz":-60},"id":"ODpHxEA0vkd7"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import random\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","import torch.nn as nn\n","from torch.nn import CrossEntropyLoss\n","\n","import transformers\n","from transformers import AutoTokenizer, AutoConfig\n","from transformers import AutoModelForSequenceClassification\n","from transformers import TFAutoModelForSequenceClassification\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n","\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","import re\n"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1702394863838,"user":{"displayName":"Ke Li","userId":"06583527288852565298"},"user_tz":-60},"id":"G2QXzlNkvkd9"},"outputs":[],"source":["class ROBERTAModel(nn.Module):\n","    \"\"\"\n","    RoBERTa model with a dropout and linear layer for binary text classification\n","    \"\"\"\n","    def __init__(self, roberta_model, num_classes=2, dropout_rate=0.3):\n","        super(ROBERTAModel, self).__init__()\n","        self.roberta = AutoModelForSequenceClassification.from_pretrained(roberta_model)\n","        self.drop = nn.Dropout(dropout_rate)\n","        self.out = nn.Linear(self.roberta.config.hidden_size, num_classes)\n","\n","    def forward(self, input_ids, attention_mask=None, labels=None):\n","        outputs = self.roberta(input_ids, attention_mask=attention_mask, labels=labels)\n","        logits = outputs.logits\n","        output = self.drop(logits)\n","        return self.out(output)"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":540,"status":"ok","timestamp":1702394868010,"user":{"displayName":"Ke Li","userId":"06583527288852565298"},"user_tz":-60},"id":"4ykvSgarp9A4"},"outputs":[],"source":["def preprocess(text):\n","  # Remove '<user>'\n","  text = re.sub(r'<user>', '', text)\n","  # Remove '<url>'\n","  text = re.sub(r'<url>', '', text)\n","  # remove numbers\n","  text = re.sub(r'\\d+', '', text)\n","  return text"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":332,"status":"ok","timestamp":1702394882990,"user":{"displayName":"Ke Li","userId":"06583527288852565298"},"user_tz":-60},"id":"f3qCUn8gvkd9"},"outputs":[],"source":["def train(data, model, optimizer, device):\n","    \"\"\"\n","        Train the model for one epoch\n","    \"\"\"\n","    model.train()\n","    running_loss = 0.0\n","\n","    for batch_idx, d in enumerate(data):\n","        mask = d['mask'].to(device, dtype=torch.long)\n","        ids = d['ids'].to(device, dtype=torch.long)\n","        token_type_ids = d['token_type_ids'].to(device, dtype=torch.long)\n","        targets = d['targets'].to(device, dtype=torch.long)\n","        optimizer.zero_grad()\n","        outputs = model(ids, mask, token_type_ids)\n","\n","        loss = torch.nn.CrossEntropyLoss(outputs, targets) # Calculate loss\n","        loss.backward()\n","        optimizer.step()\n","        if batch_idx % 10 == 0 and batch_idx !=0:\n","            temp = f'Batch index = {batch_idx}\\tRunning Loss = {running_loss/10}'\n","            print(temp)\n","            running_loss = 0.0"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":25024,"status":"ok","timestamp":1702394912393,"user":{"displayName":"Ke Li","userId":"06583527288852565298"},"user_tz":-60},"id":"6quHUiImvkd9"},"outputs":[],"source":["pos_path = 'data/twitter-datasets/train_pos_full.txt'\n","neg_path = 'data/twitter-datasets/train_neg_full.txt'\n","\n","with open(FOLDER_PATH + pos_path, 'r') as f:\n","    pos_tweets = f.readlines()\n","with open(FOLDER_PATH + neg_path, 'r') as f:\n","    neg_tweets = f.readlines()\n","\n","# preprocess data\n","pos_tweets = [preprocess(tweet) for tweet in pos_tweets]\n","neg_tweets = [preprocess(tweet) for tweet in neg_tweets]\n","\n","\n","pos_labels = [1 for _ in range(len(pos_tweets))]\n","neg_labels = [0 for _ in range(len(neg_tweets))]\n","labels = pos_labels + neg_labels\n","tweets = pos_tweets + neg_tweets\n","\n","# Combine labels and tweets into a list of tuples\n","data = list(zip(tweets, labels))\n","\n","# Shuffle the data\n","random.shuffle(data)\n","\n","# Unpack the shuffled data back into separate lists\n","train_tweets, train_labels = zip(*data)\n","\n","# Use RoBERTa tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(TWITTER_ROBERTA_MODEL)\n","config = AutoConfig.from_pretrained(TWITTER_ROBERTA_MODEL)\n","\n","# Tokenize and convert to input IDs\n","def data_generator(tweets, labels, batch_size=32):\n","    for i in range(0, len(tweets), batch_size):\n","        batch_tweets = tweets[i:i + batch_size]\n","        batch_labels = labels[i:i + batch_size]\n","        batch_encodings = tokenizer(batch_tweets, truncation=True, padding=True, max_length=MAX_LEN, return_tensors=\"pt\")\n","        yield batch_encodings, torch.tensor(batch_labels)"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":475,"status":"ok","timestamp":1702394920389,"user":{"displayName":"Ke Li","userId":"06583527288852565298"},"user_tz":-60},"id":"5HZcRenZioEa"},"outputs":[],"source":["class TweetDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R2qrdSaneVVf","outputId":"df30ee7a-a70e-4dec-ee92-237f2287a5f9"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Fold 1/5\n","Epoch 1/1\n","Batch 500 - Train loss: 0.2453528791666031, Accuracy: 0.90625\n","Batch 1000 - Train loss: 0.2500982880592346, Accuracy: 0.9375\n","Batch 1500 - Train loss: 0.440986305475235, Accuracy: 0.84375\n","Batch 2000 - Train loss: 0.20418626070022583, Accuracy: 0.875\n","Batch 2500 - Train loss: 0.297329306602478, Accuracy: 0.875\n"]}],"source":["# Set k value\n","k_folds = 5\n","kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n","\n","num_epoch = 1\n","\n","model = AutoModelForSequenceClassification.from_pretrained(TWITTER_ROBERTA_MODEL)\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","num_epochs = 1  # Number of training epochs\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model.to(device)\n","criterion = CrossEntropyLoss()\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","scheduler = get_linear_schedule_with_warmup(optimizer,\n","                      num_warmup_steps=0,\n","                      num_training_steps=len(train_labels) * (k_folds-1))\n","\n","\n","# Function to calculate accuracy\n","def calc_accuracy(preds, labels):\n","    _, predictions = torch.max(preds, dim=1)\n","    correct = (predictions == labels).float()\n","    acc = correct.sum() / len(correct)\n","    return acc\n","\n","all_labels = []\n","all_predictions = []\n","best_accuracy = 0\n","\n","for fold, (train_indices, val_indices) in enumerate(kf.split(train_tweets)):\n","    print(f\"Fold {fold + 1}/{k_folds}\")\n","\n","    # Create data loaders for the current fold\n","    train_fold_tweets = [train_tweets[i] for i in train_indices]\n","    train_fold_labels = [train_labels[i] for i in train_indices]\n","    val_fold_tweets = [train_tweets[i] for i in val_indices]\n","    val_fold_labels = [train_labels[i] for i in val_indices]\n","\n","    # Training Loop\n","    for epoch in range(num_epochs):\n","        print(f\"Epoch {epoch+1}/{num_epochs}\")\n","        model.train()\n","        total_train_loss = 0\n","        total_train_acc = 0\n","\n","        # Use the data generator for training\n","        i = 1\n","        for batch_encodings, batch_labels in data_generator(train_fold_tweets, train_fold_labels, batch_size=32):\n","            batch_encodings = {k: v.to(device) for k, v in batch_encodings.items()}\n","            batch_labels = batch_labels.to(device)\n","\n","            # Forward pass\n","            outputs = model(**batch_encodings)\n","            loss = criterion(outputs.logits, batch_labels)\n","            acc = calc_accuracy(outputs.logits, batch_labels)\n","\n","            # Backward pass\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","            optimizer.zero_grad()\n","\n","            total_train_loss += loss.item()\n","            total_train_acc += acc.item()\n","\n","            if i%500==0:\n","              print(f\"Batch {i} - Train loss: {loss.item()}, Accuracy: {acc.item()}\")\n","            i+=1\n","\n","        avg_train_loss = total_train_loss / len(train_fold_tweets)\n","        avg_train_acc = total_train_acc / len(train_fold_tweets)\n","        print(f\"Fold {fold+1} - Train loss: {avg_train_loss}, Accuracy: {avg_train_acc}\")\n","\n","    # Validation Loop\n","    model.eval()\n","    total_val_accuracy = 0\n","    total_val_loss = 0\n","\n","    for batch_encodings, batch_labels in data_generator(val_fold_tweets, val_fold_labels, batch_size=32):\n","        with torch.no_grad():\n","            batch_encodings = {k: v.to(device) for k, v in batch_encodings.items()}\n","            batch_labels = batch_labels.to(device)\n","\n","            outputs = model(**batch_encodings)\n","            loss = criterion(outputs.logits, batch_labels)\n","            acc = calc_accuracy(outputs.logits, batch_labels)\n","\n","            total_val_loss += loss.item()\n","            total_val_accuracy += acc.item()\n","\n","            # for overall metrics calculation\n","            preds = outputs.logits.argmax(dim=1).cpu().numpy()\n","            labels = batch_labels.cpu().numpy()\n","            all_predictions.extend(preds)\n","            all_labels.extend(labels)\n","\n","    avg_val_loss = total_val_loss / len(val_fold_tweets)\n","    avg_val_accuracy = total_val_accuracy / len(val_fold_tweets)\n","    print(f\"Fold {fold+1} - Validation loss: {avg_val_loss}, Accuracy: {avg_val_accuracy}\")\n","    if avg_val_accuracy>best_accuracy:\n","      torch.save(model, FOLDER_PATH + 'manipulated/roberta.pth')\n","\n","# After all folds, calculate overall metrics\n","precision = precision_score(all_labels, all_predictions, average='macro')\n","recall = recall_score(all_labels, all_predictions, average='macro')\n","f1 = f1_score(all_labels, all_predictions, average='macro')\n","\n","print(\"Training complete!\")\n","print(f\"Overall Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")"]},{"cell_type":"markdown","metadata":{"id":"EFuJT7aSioEc"},"source":["### Possible improvement\n","1. Don't store all the scores in one variable, just store for each epoch and only keep the average score\n","2. save model for Each epoch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1phcjpJ7vkd-"},"outputs":[],"source":["torch.save(model, FOLDER_PATH + 'manipulated/roberta_final.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kvt6507Ha_sp"},"outputs":[],"source":["## predict\n","model = torch.load(FOLDER_PATH + 'manipulated/bert.pth')\n","\n","## use the test set\n","test_path = FOLDER_PATH +'data/twitter-datasets/test_data.txt'\n","with open(test_path, 'r') as f:\n","    test_tweets = f.readlines()\n","\n","test_encodings = tokenizer(test_tweets, truncation=True, padding=True, max_length=MAX_LEN)\n","test_dataset = TweetDataset(test_encodings, [0 for _ in range(len(test_tweets))])\n","test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n","\n","\n","model.eval()\n","predictions = []\n","for batch in test_loader:\n","    with torch.no_grad():\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        outputs = model(**batch)\n","        preds = outputs.logits.argmax(dim=1).cpu().numpy()\n","        predictions.extend(preds)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b6r6SXzwbgee"},"outputs":[],"source":["predictions = np.array(predictions)\n","predictions[predictions == 0] = -1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m_iHW_xrbVlm"},"outputs":[],"source":["print(predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zG1C1sVVbKZp"},"outputs":[],"source":["submission = pd.DataFrame({'Id':range(1, len(predictions) + 1),'Prediction': predictions})\n","submission.to_csv(FOLDER_PATH + 'manipulated/roberta_submission.csv', index=False)"]}],"metadata":{"accelerator":"TPU","colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}